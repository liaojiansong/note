## mysql 笔记

### sql 的执行过程

* 流程一:连接器->分析器->查询缓存

* 流程二:连接器->分析器->优化器->执行器->引擎->返回结果

#### 连接器

* 查看当前连接信息: `show processlist`
* mysql 执行过程中的临时内存是存储在连接对象当中的,长期连接可能会导致内存占用过大
* `mysql_reset_connrction` 可以重置所有连接,降低内存占用

#### 查询缓存

使用查询缓存弊大于利

* 随sql的改变命中率极低,数据更新也会清除缓存
* mysql8.0 将废弃查询缓存

#### 分析器

分析词法和语法,通过后交给优化器

#### 优化器

* 寻找合适的索引
* 寻找合适的连表顺序
* 在联合索引中,如果应用程序写的查询顺序与预定义的顺序不一致,优化器也会自动进行优化

#### 执行器

* 权限校验
* `rows_examined` 扫描行数,扫描记录,将符合条件的放入结果集合中

### 日志模块

#### redolog (重做日志)

mysql在更新数据的时候不会马上就去磁盘查找更新,这这样效率低下.而是先写到临时日志上,等数据库空闲了在写到磁盘上,这种技术归纳为:先写日志,再写磁盘即WAL(Write-Ahead Logging)

* redolog文件是一个循环链表,写完了又从头开始写
* redolog 是innodb特有的,是引擎层面上的东西
* redolog 是顺序写入,不用找,相对较快
* redolog描述了怎么改数据

#### binlog (归档日志)

binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”,binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

#### binlog的三种格式

##### statement

SQL 语句的原文,对于一些不指定id的更新操作,会无法恢复数据

##### row

row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。

##### mixed

mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。



* statement: SQL 语句的原文,对于一些不指定id的更新操作,会无法恢复数据
* row
* 
* mixed

#### redolog 和binlog 配合流程

1. 执行器调用引擎接口,取出记录id=2,money=100 这一行
2. 执行器将money+=10后,调用引擎接口写回去
3. 引擎更新内存,并将这个更新写到redolog当中,此时redolog处于prepare状态
4. 执行器生成binlog,并写入磁盘
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。
6. binlog描述数据更改的前后详情

### 隔离性与隔离级别

事务的几大特性,原子性.一致性.隔离性,持久性

#### 隔离级别

* 读未提交:别的事务还没提交的值被你读到了(脏读)
* 读已提交:开始读的时候是这样,这中间别的事务提交了,读取到的又是那样了(不可重复读)
* 可重复读:一个事务执行过程中看到了数据总是和他开始看到的数据是一致的.但是自己提交后再去读的时候发现这个值跟执行过程的值不一样了,因为别的事务改了(幻读)
* 串行化:事务之间顺序执行

#### 隔离级别创建视图的时机

数据库里面会创建一个视图，没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”.访问的时候以视图的逻辑结果为准

* 读未提交:不存在视图,立马执行语句得到结果并返回
* 读已提交:每个 SQL 语句开始执行的时候创建的
* 可重复读:事务启动时创建的，整个事务存在期间都用这个视图
* 串行化:直接用加锁的方式来避免并行访问

### 索引

#### 哈希表索引

哈希表这种结构适用于只有等值查询的场景,如果要求一段范围的就歇菜了,不如直接使用全表扫描

#### B+树索引

在innodb中,B+树的高度很低,一般为三,叶子节点之间带有指针,方便范围查找

#### 索引分类

根据B+树上的叶子节点的内容,可以将索引分为聚簇索引和二级索引

##### 聚簇索引（clustered index）|主键索引

主键索引的**叶子节点**上存的是整整一页的数据,换个方向来说的话就是在innodb中,数据是存储在主键索引上的

* 因为主键索引的有序性,所以数据变动的时候,需要维护索引树的有序性

* 对于没有主键的表,innodb 会默认创建一个Rowid的主键,占六个字节

* B+树的叶子节点是page （页），一个页里面可以存多个行

  

##### 二级索引

二级索引的叶子节点上存的是主键的值.当扫描索引树的时候,要想获取完整的数据还得拿主键的值去主键索引上查询,这一过程称为回表

* 删除主键索引会导致二级索引失效
* 主键长度越小,相应的叶子节点也就越小,对应占用空间就小了

索引可能因为删除，或者页分裂等原因，导致数据页有空洞,重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间

#### 索引妙用

合理利用索引,可以大大地提高查询性能

##### 覆盖索引

如果二级索引叶子节点上就包含了我们需要的信息,那么就不需要回表查询,这种技术称为**覆盖索引**

##### 最左前缀原则

对于联合索引,查询的时候会从左到右进行匹配索引树. 合理安排联合索引的字段顺序,可以让我们少维护索引,减低空间占用,提高数据更新性能

##### 索引下推优化（index condition pushdown)

在联合索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数

` select age,name,address from student where age = 18 and name like '%敏%'`

假设我们的有`index idx_1(age,name)`,原则上是不能用尽联合索引的,但是基于索引下推的优化流程如下,

1. 在辅助索引中根据`age` 找到索引记录`(18,'王八蛋')`
2. 尝试where语句的筛选在不在索引中,在的话直接判断name是否符合`like '%敏%'``
3. ......

##### 联合索引数据结构是怎样的?

联合索引(a,b,c)，其实质是按a,b,c的顺序拼接成了一个二进制字节数组,索引记录是按该字节数组逐字节比较排序的

##### 给字符串加索引

假设我们现在有这样一批数据

| id   | name | email          |
| ---- | ---- | -------------- |
| 1    | jack | jack@gmail.com |
| 2    | mark | mark@gmail.com |
| 3    | bob  | bob@gmail.com  |

加入我们要为email建立索引,但是又要节省空间,我们可以只为前面的几个字符构建索引即可,因为后面的邮箱后缀是一样的,区分度不大. 构建语句如下:

`` alter table users add index idx_email(email(5));`` 

创建前缀索引，节省空间，但会增加查询扫描次数，并且**不能使用覆盖索引**；

##### 不要对索引字段做函数操作

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走**树搜索功能**，需要注意的是，优化器并不是要放弃使用这个索引。优化器可能选择**遍历索引树**，逐条数据地代入函数，获取符合的结果

**索引字段不能进行函数操作，但是索引字段的参数可以玩函数**

##### 索引类型与输入类型要完全匹配

索引类型应该与约束语句中输入的类型保持完全一致，否则会导致内部采用函数进行转换，从而丧失了利用索引有序性的功能．在进行连表查询的时候，要确保关联字段的编码要保持完全一致，避免隐式的函数转换过程

### 各种Buffer

#### change buffer

change buffer 用于在内存中临时记录对数据的修改操作,而不是将修改立马写入到内存中,减少磁盘的IO次数,从而提升性能

虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge,触发merger的情况有以下几种

1. 系统在后台有专门线程会定期merge
2. 进程关闭时会merge
3. 访问这个数据页的时候会merger

##### change buffer 与唯一索引和普通索引之间的关系

**事先强调:当修改数据时,要保证唯一索引的唯一性,必然要从存储好的数据中取出来判断新的值是否重复**

插入一条新的数据时(A=300)

唯一索引:

> 1. 将数据页从磁盘读到内存当中
> 2. 判读是否冲突,不冲突则直接修改内存,执行完毕

普通索引

> 1. 直接将更新记录写到change buffer 中,语句执行结束

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

结论:普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，建议尽量选择普通索引

##### change buffer 和 redolog 之间的区别和联系

change buffer做修改时需要写redo，在做恢复时需要根据redo来恢复change buffer 

redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。

#### sort_buffer(排序缓存)

MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。

##### 全字段排序过程

假设我们现在有一张这样的表

| id   | city | name | age  | addr |
| ---- | ---- | ---- | ---- | ---- |
| 1    | 广州 | 小红 | 18   |      |
| 2    | 深圳 | 小白 | 15   |      |
| 3    | 深圳 | 小李 | 23   |      |
|      |      |      |      |      |

假设我们要执行这样一条语句:

``select city,name,age from t where city='杭州' order by name limit 1000 ;``

那么他的过程如下:

1. 初始化 `sort_buffer`,确定我们select的name,age,city 这三个字段
2. 根据索引city找到满足条件的记录id
3. 根据主键id回表找到这条记录取出name,age,city存入sort_buffer
4. 重复3,知道找不到满足条件的记录为止
5. 对sort_buffer中的数据按照name快速排序
6. 对排序结果取前面1000条返回

需要的注意的是sort_buffer是有大小限制的,如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

##### rowid排序过程

全字段排序中,会将一些与排序无关的字段也放入到sort_buffer中,有时会超出进而导致mysql采用临时文件排序,导致性能下降.

既然全字段排序会浪费内存,那么我们可不可以只拿关键字段进行排序,然后再回表查询输出呢?答案是肯定的,这就是rowid排序

**max_length_for_sort_data**是 MySQL 中专门控制用于排序的行数据的长度的一个参数(默认1024)。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。

还是`select city,name,age from t where city='杭州' order by name limit 1000 ;`我们看看rowid执行排序的过程:

1. 初始化 sort_buffer，确定放入两个字段，即 name 和 id；
2. 根据索引city找到满足条件的记录id
3. 根据主键id回表找到这条记录取出name,id存入sort_buffer
4. 从索引 city 取下一个记录的主键 id；
5. 重复3,知道找不到满足条件的记录为止
6. 对 sort_buffer 中的数据按照字段 name 进行排序；
7. 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。

需要说明的是，最后的“结果集”是一个逻辑概念，实际上 MySQL 服务端从排序后的 sort_buffer 中依次取出 id，然后到原表查到 city、name 和 age 这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。

##### 全字段和rowid排序该怎么选?

从上面的过程对比我们可以看到,rowid排序多了一步回表的过程,因此效率更加低,不会被优先选择

##### 利用覆盖索引获取天然排好序的数据

我们知道在B+树上,叶子节点是顺序排列的,如果我们想要的字段包含在索引上,那么直接取索引即可,不需要回表进行查询

覆盖索引情况有:

###### 部分覆盖

例如我们要name,city,age 三个字段,于是我们建立索引`idx(city,name)`,这种情况下,流程为:

1. 从索引 (city,name) 找到第一个满足 city='杭州’条件的主键 id；
2. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回；
3. 从索引 (city,name) 取下一个记录主键 id；
4. 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。

##### 全部覆盖

例如我们要name,city,age 三个字段,那么我们干脆建立联合索引`idx(city,name,age)`,覆盖全部查询的字段,这种情况下,流程为:

1. 从索引 (city,name,age) 找到第一个满足 city='杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；
2. 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；
3. 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。

#### join buffer(连表buffer)

```mysql
# t1
CREATE TABLE `t1` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`)) ENGINE=InnoDB;
# t2
CREATE TABLE `t2` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`)) ENGINE=InnoDB;
```

这两个表都有一个主键索引 id 和一个索引 a，字段 b 上无索引。t1插入100条数据,t2插入1000条数据;

##### Index Nested-Loop Join(索引嵌套循环联接)

下面我们来看看执行`select * from t1 straight_join t2 on (t1.a=t2.a);`语句的流程,首先先看explain的结果:

| id   | select\_type | table | partitions | type | possible\_keys | key  | key\_len | ref      | rows | filtered | Extra       |
| :--- | :----------- | :---- | :--------- | :--- | :------------- | :--- | :------- | :------- | :--- | :------- | :---------- |
| 1    | SIMPLE       | t1    | NULL       | ALL  | a              | NULL | NULL     | NULL     | 100  | 100      | Using where |
| 1    | SIMPLE       | t2    | NULL       | ref  | a              | a    | 5        | jie.t1.a | 1    | 100      | NULL        |

在这个语句中,被驱动表t2.a上有索引,连接过程中用上了这个索引,具体执行流程如下:

1. 从t1中读取一行数据,记为:R
2. 从R中取出a字段到t2中查找
3. 获取t2中满足条件的行,跟R组成一行并放入结果集当中
4. 重复执行1到3,直到t1扫描完毕

在这一个流程中:

1. 对驱动表t1扫描了100行
2. 在对t2进行匹配的过程中,因为走的是索引树,所以每次搜索只扫描了一行,所以扫描行数也是100行
3. 总共扫描200行

在这个join的过程中,驱动表走了全表扫描,而被驱动表走的是索引树搜索

#### Simple Nested-Loop Join(简单嵌套循环联接)

执行这个语句`select * from t1 straight_join t2 on (t1.a=t2.b);` 注意b字段是没有索引的. 执行流程如下:

1. 依次扫描t1,每次取一行数据R
2. 拿R的a字段去扫描t2表,这个过程扫描1000行
3. 总共扫描100*1000次

这种算法效率低下,Mysql并没有使用

#### Block Nested-Loop Join(分块嵌套循环联接)

执行这个语句`select * from t1 straight_join t2 on (t1.a=t2.b);` 注意b字段是没有索引的,被驱动表上没有可用的索引，算法的流程是这样的：

1. 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存；
2. 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回\

在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。

加入现在的情况是:t1 是一个大表,`join_buffer ` 放不下

join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。执行过程就变成了：

1. 扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步；
2. 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回；
3. 清空 join_buffer；
4. 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。

缺点:

1. 可能会多次扫描被驱动表，占用磁盘 IO 资源；
2. 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；
3. 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。

#### 无索引下join的执行顺序和优化策略

前提条件b字段无索引,在这条`select * from t1 join t2 on (t1.b=t2.b) where t2.b>=1 and t2.b<=2000;`中,执行流程如下:

1. 把表 t1 的所有字段取出来，存入 join_buffer 中。
2. 扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比，如果不满足t1.b=t2.b则跳过,否则再判断其他条件,满足条件就返回

优化策略就是**BNL 转 BKA,**例如使用临时表,具体流程如下:

1. 把表 t2 中满足条件的数据放在临时表 tmp_t 中；
2. 为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引；
3. 让表 t1 和 tmp_t 做 join 操作。

#### 脏页

我们知道,对数据的修改不会立马写入到磁盘中,而是将更改暂时写入到内存数据页和redolog中,当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。将脏页同步到磁盘上的过程叫做flush,也就是刷脏页

什么时候会刷脏页呢?

1. redolog 快写满的时候
2. 内存空间不足,需要淘汰脏页让出内存空间
3. 空闲的时候,系统自动刷
4. 关机的时候

常见问题:有时候mysql的发生抖动,或者内存占用过高这种情况的原因有

1. 数据库正在刷脏页
2. 刷脏页的速度太慢,闸门开得太小,会导致排队处理脏页,占内存,耗IO

### 锁锁锁

#### 全局锁

全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

当连接异常断开的时候,这个锁会自动释放

##### 应用场景

全库逻辑备份

#### 表锁

##### 普通表锁

##### MDL（metadata lock)

MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。当对一个表做增删改查操作的时候，加 **MDL 读锁**；当要对表做结构变更操作的时候，加**MDL 写锁**

#### 行锁

行锁由引擎实现,目前innodb是支持行锁的

#### 两阶段锁协议

在innodb的事务中,行锁是**需要的时候才加上**的,但是要**等到事务结束后**才释放,不是马上释放,这就是两阶段锁协议

优化建议:在一次对多个表多条记录的更新中,优先执行低频访问的行操作最后执行高频访问行的操作,这样可以减低锁的占用时间

### Innodb

InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB



### 表文件的管理

##### 删除记录表占用空间没减少?

InnoDB 里的数据都是用 B+ 树的结构组织的,我们要删掉某一条记录时候，InnoDB 引擎只会把  这个记录标记为删除,可能会复用这个位置,并不会重新组织索引,所以表空间没有减少

##### 空洞是什么?

就是指哪一些记录删除后可以复用但却没复用到的位置叫做空洞,这种空洞会一直占据空间,浪费磁盘

当然插入数据的时候,会造成页分裂,同样可能产生空洞

也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而**重建表**，就可以达到这样的目的。

##### 重建表

使用`alter table A engine=InnoDB`来重建表,5.5以后的版本可以在线重建表,重建的流程如下:

1. 建立一个临时文件，扫描表 A 主键的所有数据页
2. 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；
3. 成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中
4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件
5. 用临时文件替换表 A 的数据文件。

### 主备一致

主库会将自己的binlog通过专门的线程传递给备库,备库执行binlog即可

### 了解一下

#### mysql 是边读边发送

假设现在要全表扫描一个200G的表并发送到客户端,而物理机的内存只有8G,那么内存会不会爆满呢?

答案是不会的,因为mysql是边读边发送,具体看流程如下:

1. 获取一行结果集,并存入`net_buffer`中
2. 重复步骤1,直到net_buffer`满为止,然后调用网络接口发送出去
3. 如果发送成功,就继续1,2步,直到数据发送完毕
4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。

`net_buffer` 这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k. `socket send buffer` 负责向网络发送数据,如果``socket send buffer`` 是满的状态,前面的扫描也得暂停等待

#### send state

当我们执行`show processlist` 可能会在State列看到以下两种描述:

* `Sending to client`:表示正在调用网络接口给客户端发送数据
* `Sending data` 执行器正在执行语句,有可能是在等待锁

#### 全表扫描对 InnoDB 的影响

当我们执行全表扫描的时候,必然会从磁盘读取数据页载入`buffer_pool`中,然而`buffer_pool`(由innodb_buffer_pool_size控制)是有限的,新的数据进入必然会淘汰旧的数据页. InnoDB的内存淘汰算法采用了LRU(Least Recently Used)算法

如果单纯的使用LRU算法会引发一个问题,就是但我们扫描一个大表的数据进入`buffer_pool`时候,可能会把我们一些热点数据的缓存淘汰出去,造成`buffer_poll`命中率急剧下降,磁盘压力增加,sql响应缓慢

对此InnoDB在LRU的实现上,按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域,频繁被替换的数据放在old区,用于接纳缓冲全表扫描的数据. 而young区则会继续存放热点数据,保证不会被全表扫描的数据淘汰









